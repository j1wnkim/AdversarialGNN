{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jik19004/anaconda3/envs/Privacy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import torch \n",
    "from models import NodeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.datasets import Planetoid \n",
    "\n",
    "\n",
    "cora_dataset = Planetoid(root = \"/home/jik19004/FilesToRun/AdversarialGNN\", name = \"Cora\", split = \"public\") \n",
    "pubmed_dataset= Planetoid(root = \"/home/jik19004/FilesToRun/AdversarialGNN\", name = \"PubMed\", split = \"public\") \n",
    "citeseer_dataset = Planetoid(root = \"/home/jik19004/FilesToRun/AdversarialGNN\", name = \"CiteSeer\", split = \"public\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora has 7 number of classes\n",
      "\n",
      "x\n",
      "edge_index\n",
      "y\n",
      "train_mask\n",
      "val_mask\n",
      "test_mask\n",
      "\n",
      "\n",
      "The length of cora train mask: 140\n",
      "The length of cora validation mask: 500\n",
      "The length of cora test mask: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jik19004/anaconda3/envs/Privacy/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "print(\"Cora has {} number of classes\".format((cora_dataset.num_classes)))\n",
    "print(\"\")\n",
    "for key, value in cora_dataset.data: \n",
    "    print(key)\n",
    "    \n",
    "print(\"\\n\\nThe length of cora train mask: {}\".format(sum(cora_dataset.data[\"train_mask\"])))\n",
    "print(\"The length of cora validation mask: {}\".format(sum(cora_dataset.data[\"val_mask\"])))\n",
    "print(\"The length of cora test mask: {}\".format(sum(cora_dataset.data[\"test_mask\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "train_mask = cora_dataset.data[\"train_mask\"]\n",
    "val_mask = cora_dataset.data[\"val_mask\"]\n",
    "test_mask = cora_dataset.data[\"test_mask\"]\n",
    "\n",
    "train_cora_x = cora_dataset.data[\"x\"][train_mask]\n",
    "train_cora_y = F.one_hot(cora_dataset.data[\"y\"][train_mask], num_classes = 7)\n",
    "\n",
    "val_cora_x = cora_dataset.data[\"x\"][val_mask]\n",
    "val_cora_y = F.one_hot(cora_dataset.data[\"y\"][val_mask], num_classes = 7)\n",
    "\n",
    "test_cora_x = cora_dataset.data[\"x\"][test_mask]\n",
    "test_cora_y = F.one_hot(cora_dataset.data[\"y\"][test_mask], num_classes = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0])\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from mechanisms import RandomizedResopnse, MultiBit \n",
    "from models import NodeClassifier \n",
    "\n",
    "eps = [0.1, 0.2, 0.5, 1, 1.5, 2]\n",
    "random_response = RandomizedResopnse(eps[4],7)\n",
    "print(random_response(test_cora_y[0]))\n",
    "print(test_cora_y[0])\n",
    "\n",
    "multibit = MultiBit(eps = 0.1,input_range = (100, 500))\n",
    "print(multibit.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains loops?: False\n",
      "The dataset contains isolated nodes?: False\n",
      "The dataset is directed?: False\n",
      "Number of edgwes in the dataset: 10556\n",
      "The number of average degree in the nodes: 3.8980797636632203\n",
      "[[  30   30   30   30   30   30]\n",
      " [ 697  738 1358 1416 2162 2343]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jik19004/anaconda3/envs/Privacy/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'contains_self_loops' is deprecated, use 'has_self_loops' instead\n",
      "  warnings.warn(out)\n",
      "/home/jik19004/anaconda3/envs/Privacy/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "edge_index = cora_dataset.data[\"edge_index\"].numpy()\n",
    "\n",
    "print(\"The dataset contains loops?: {}\".format(cora_dataset.data.contains_self_loops()))\n",
    "print(\"The dataset contains isolated nodes?: {}\".format(cora_dataset.data.contains_isolated_nodes()))\n",
    "print(\"The dataset is directed?: {}\".format(cora_dataset.data.is_directed()))\n",
    "print(\"Number of edgwes in the dataset: {}\".format(cora_dataset.data.num_edges))    \n",
    "print(\"The number of average degree in the nodes: {}\".format(cora_dataset.data.num_edges/cora_dataset.data.num_nodes))  \n",
    "print(edge_index[:, np.where(edge_index[0]==30)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing Data and Perturbing our Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_dataset = Planetoid(root = \"/home/jik19004/FilesToRun/AdversarialGNN\", name = \"Cora\", split = \"public\")\n",
    "cora_dataset = cora_dataset.data\n",
    "\n",
    "labeled_nodes = torch.where(cora_dataset.y >= 0)[0]  # Get indices of labeled nodes\n",
    "num_labeled = labeled_nodes.size(0)\n",
    "perm = torch.randperm(num_labeled)  # Random permutation of indices\n",
    "\n",
    "train_split = int(0.5 * num_labeled)\n",
    "val_split = int(0.25 * num_labeled)\n",
    "\n",
    "train_idx = labeled_nodes[perm[:train_split]]\n",
    "val_idx = labeled_nodes[perm[train_split:train_split + val_split]]\n",
    "test_idx = labeled_nodes[perm[train_split + val_split:]]\n",
    "\n",
    "cora_dataset.train_mask = torch.zeros(cora_dataset.num_nodes, dtype=torch.bool)\n",
    "cora_dataset.val_mask = torch.zeros(cora_dataset.num_nodes, dtype=torch.bool)\n",
    "cora_dataset.test_mask = torch.zeros(cora_dataset.num_nodes, dtype=torch.bool)\n",
    "cora_dataset.train_mask[train_idx] = True\n",
    "cora_dataset.val_mask[val_idx] = True\n",
    "cora_dataset.test_mask[test_idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708, 7], train_mask=[2708], val_mask=[2708], test_mask=[2708], T=[7, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jik19004/anaconda3/envs/Privacy/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transforms import FeaturePerturbation, LabelPerturbation \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "\n",
    "cora_dataset = Planetoid(root = \"/home/jik19004/FilesToRun/AdversarialGNN\", name = \"Cora\", split = \"public\")\n",
    "cora_dataset = cora_dataset.data \n",
    "\n",
    "#### spliting the data \n",
    "labeled_nodes = torch.where(cora_dataset.y >= 0)[0]  # Get indices of labeled nodes\n",
    "num_labeled = labeled_nodes.size(0)\n",
    "perm = torch.randperm(num_labeled)  # Random permutation of indices\n",
    "\n",
    "train_split = int(0.5 * num_labeled)\n",
    "val_split = int(0.25 * num_labeled)\n",
    "\n",
    "train_idx = labeled_nodes[perm[:train_split]]\n",
    "val_idx = labeled_nodes[perm[train_split:train_split + val_split]]\n",
    "test_idx = labeled_nodes[perm[train_split + val_split:]]\n",
    "\n",
    "cora_dataset.train_mask = torch.zeros(cora_dataset.num_nodes, dtype=torch.bool)\n",
    "cora_dataset.val_mask = torch.zeros(cora_dataset.num_nodes, dtype=torch.bool)\n",
    "cora_dataset.test_mask = torch.zeros(cora_dataset.num_nodes, dtype=torch.bool)\n",
    "cora_dataset.train_mask[train_idx] = True\n",
    "cora_dataset.val_mask[val_idx] = True\n",
    "cora_dataset.test_mask[test_idx] = True ### modifying our train, val, and test partitions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "data = cora_dataset.x \n",
    "train_mask = cora_dataset.train_mask \n",
    "val_mask = cora_dataset.val_mask \n",
    "test_mask = cora_dataset.test_mask \n",
    "\n",
    "train_data = data[train_mask].numpy() \n",
    "val_data = data[val_mask].numpy() \n",
    "test_data = data[test_mask].numpy() \n",
    "\n",
    "scaler.fit(train_data)\n",
    "train_data = torch.Tensor(scaler.transform(train_data)) # Standardize our data into a fixed range.\n",
    "val_data = torch.Tensor(scaler.transform(val_data))\n",
    "test_data = torch.Tensor(scaler.transform(test_data))\n",
    "\n",
    "data[train_mask] = train_data\n",
    "data[val_mask] = val_data\n",
    "data[test_mask] = test_data \n",
    "cora_dataset.x = data  \n",
    "\n",
    "feature_preprocess = FeaturePerturbation(mechanism = \"mbm\", x_eps = 0.1)\n",
    "cora_transform = feature_preprocess(cora_dataset) \n",
    "cora_transform = LabelPerturbation(y_eps = 2)(cora_transform)\n",
    "print(cora_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv \n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math \n",
    "def Train_and_Evaluate(data, num_epochs, num_callbacks, criterion, device, y_eps = 2, num_classes = 7): \n",
    "    model = NodeClassifier(input_dim = 1433, model = \"gcn\", hidden_dim = 16, num_classes = 7, dropout = 0.01, \n",
    "                       x_steps = 4, y_steps =2, forward_correction = True)\n",
    "# try to set the privacy epsilon budget to something that is very high, play around with hyper parameters. \n",
    "# try to play around with different splits of the training, validation, and testing partitions. \n",
    "    train_mask = data.train_mask \n",
    "    val_mask = data.val_mask \n",
    "    \n",
    "    optimizer = torch.optim.Adam(params = model.parameters(),lr = 0.001, weight_decay = 0.001)\n",
    "    training_output = data.y[train_mask].float() \n",
    "    validation_output = data.y[val_mask].float() \n",
    "    \n",
    "    training_loss_list = [] \n",
    "    validation_loss_list = [] \n",
    "    \n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    training_output = training_output.to(device)\n",
    "    validation_output = validation_output.to(device)\n",
    "    val_accuracy = 0 \n",
    "    train_accuracy = 0 \n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = np.inf \n",
    "    limit_val_accuracy = math.e.__pow__(y_eps)/(math.e.__pow__(y_eps) + num_classes - 1) * 100\n",
    "    \n",
    "\n",
    "    callback = 0 \n",
    "    train_loss_val = 0 \n",
    "    best_loss_val = np.inf \n",
    "    current_validation_loss = 0 \n",
    "    for i in range(num_epochs): \n",
    "        model.train()\n",
    "        optimizer.zero_grad()  \n",
    "        training_loss, train_metrics = model.training_step(data) \n",
    "        training_loss.backward() \n",
    "        optimizer.step() # have the optimizer update the weights. \n",
    "        training_loss_list.append(training_loss.item()) \n",
    "        train_loss_val = training_loss.item()  \n",
    "        train_accuracy = train_metrics[\"train/acc\"]\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss, val_metrics = model.validation_step(data) \n",
    "            validation_loss_list.append(val_loss) \n",
    "            current_validation_loss = val_loss \n",
    "            val_accuracy = val_metrics[\"val/acc\"]\n",
    "        \n",
    "        if (i % 10 == 0):\n",
    "            print(\"Training loss at epoch {}: {}. Validation loss: {}\".format(i, train_loss_val, current_validation_loss))  \n",
    "        \n",
    "        if val_loss < best_val_loss and val_accuracy <= limit_val_accuracy and train_accuracy <= limit_val_accuracy:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model, \"/home/jik19004/FilesToRun/AdversarialGNN/LPGNN\")\n",
    "            callback = 0 \n",
    "            \n",
    "        else: \n",
    "            callback+=1 \n",
    "            if callback >= num_callbacks: \n",
    "                break \n",
    "def Evaluate(model, data, device):\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    loss, metrics = model.validation_step(data)\n",
    "    print(metrics[\"test/acc\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "def Train_and_Evaluate_normal(model, data, num_epochs, num_callbacks, criterion, device): \n",
    "    train_mask = data.train_mask \n",
    "    val_mask = data.val_mask \n",
    "    \n",
    "    optimizer = torch.optim.Adam(params = model.parameters())\n",
    "    training_output = data.y[train_mask].float() \n",
    "    validation_output = data.y[val_mask].float() \n",
    "    LossFunc = criterion() \n",
    "    \n",
    "    training_loss_list = [] \n",
    "    validation_loss_list = [] \n",
    "    \n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    training_output = training_output.to(device)\n",
    "    validation_output = validation_output.to(device)\n",
    "    \n",
    "    callback = 0 \n",
    "    train_loss_val = 0 \n",
    "    best_loss_val = np.inf \n",
    "    current_validation_loss = 0 \n",
    "    for i in range(num_epochs): \n",
    "        model.train() \n",
    "        predictedValues = model(data.x, data.edge_index) \n",
    "        training_predictions = predictedValues[train_mask] \n",
    "        optimizer.zero_grad() \n",
    "        training_loss = LossFunc(training_predictions, training_output) \n",
    "        training_loss.backward() \n",
    "        optimizer.step() # have the optimizer update the weights. \n",
    "        training_loss_list.append(training_loss.item()) \n",
    "        train_loss_val = training_loss.item()  \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval() \n",
    "            predictedValues = model(data.x, data.edge_index)\n",
    "            predictedValues = predictedValues[val_mask]\n",
    "            val_loss = LossFunc(predictedValues, validation_output)\n",
    "            validation_loss_list.append(val_loss.item()) \n",
    "            current_validation_loss = val_loss.item() \n",
    "        \n",
    "        if (i % 10 == 0):\n",
    "            print(\"Training loss at epoch {}: {}. Validation loss: {}\".format(i, train_loss_val, current_validation_loss))            \n",
    "            \n",
    "        if current_validation_loss < best_loss_val and train_loss_val < best_loss_val: \n",
    "            best_loss_val = current_validation_loss \n",
    "            torch.save(model, \"/home/jik19004/FilesToRun/AdversarialGNN/LPGNN_normal\")\n",
    "            callback = 0 \n",
    "        else: \n",
    "            callback+=1 \n",
    "            if callback >= num_callbacks: \n",
    "                break \n",
    "def Evaluate_normal(model, data, loss, device):\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    test_mask = data.test_mask \n",
    "    prediction = model(data.x, data.edge_index)\n",
    "    test_preds = prediction[test_mask]\n",
    "    test_output = data.y[test_mask]\n",
    "    test_numeric = test_output.argmax(dim = 1)\n",
    "    _, test_preds_max = test_preds.max(dim = 1)\n",
    "    correct = (test_preds_max == test_numeric).sum().item()  # Count correct predictions\n",
    "\n",
    "    rate = correct/len(test_output)\n",
    "    LossFunc = loss() \n",
    "    return LossFunc(test_preds, test_output), rate \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 0: 1.8882899284362793. Validation loss: 1.9425396919250488\n",
      "Training loss at epoch 10: 1.7497422695159912. Validation loss: 1.9324451684951782\n",
      "Training loss at epoch 20: 1.6503822803497314. Validation loss: 1.9229793548583984\n",
      "Training loss at epoch 30: 1.566489338874817. Validation loss: 1.9139456748962402\n",
      "Training loss at epoch 40: 1.4923007488250732. Validation loss: 1.904600977897644\n",
      "Training loss at epoch 50: 1.424039363861084. Validation loss: 1.8946433067321777\n",
      "Training loss at epoch 60: 1.3640096187591553. Validation loss: 1.8845158815383911\n",
      "Training loss at epoch 70: 1.3092103004455566. Validation loss: 1.8745923042297363\n",
      "Training loss at epoch 80: 1.2603241205215454. Validation loss: 1.8650163412094116\n",
      "Training loss at epoch 90: 1.2161146402359009. Validation loss: 1.8557467460632324\n",
      "Training loss at epoch 100: 1.1771162748336792. Validation loss: 1.8467885255813599\n",
      "Training loss at epoch 110: 1.1407994031906128. Validation loss: 1.838183045387268\n",
      "Training loss at epoch 120: 1.1082875728607178. Validation loss: 1.8299965858459473\n",
      "Training loss at epoch 130: 1.0799843072891235. Validation loss: 1.8222544193267822\n",
      "Training loss at epoch 140: 1.0532944202423096. Validation loss: 1.8149590492248535\n",
      "Training loss at epoch 150: 1.030261754989624. Validation loss: 1.8081485033035278\n",
      "Training loss at epoch 160: 1.0092558860778809. Validation loss: 1.8018832206726074\n",
      "Training loss at epoch 170: 0.9908726811408997. Validation loss: 1.7961394786834717\n",
      "Training loss at epoch 180: 0.9745072722434998. Validation loss: 1.7908680438995361\n",
      "Training loss at epoch 190: 0.958613932132721. Validation loss: 1.7860031127929688\n",
      "Training loss at epoch 200: 0.9440851807594299. Validation loss: 1.7814948558807373\n",
      "Training loss at epoch 210: 0.9319828152656555. Validation loss: 1.777344822883606\n",
      "Training loss at epoch 220: 0.919086754322052. Validation loss: 1.7735599279403687\n",
      "Training loss at epoch 230: 0.9077391028404236. Validation loss: 1.7700196504592896\n",
      "Training loss at epoch 240: 0.8977800011634827. Validation loss: 1.7668006420135498\n",
      "Training loss at epoch 250: 0.8883509039878845. Validation loss: 1.7638301849365234\n",
      "Training loss at epoch 260: 0.8786802291870117. Validation loss: 1.76103675365448\n",
      "Training loss at epoch 270: 0.8698409199714661. Validation loss: 1.7584365606307983\n",
      "Training loss at epoch 280: 0.861282229423523. Validation loss: 1.75603187084198\n",
      "Training loss at epoch 290: 0.8536363244056702. Validation loss: 1.7537367343902588\n",
      "Training loss at epoch 300: 0.845745325088501. Validation loss: 1.751507043838501\n",
      "Training loss at epoch 310: 0.8380319476127625. Validation loss: 1.7493411302566528\n",
      "Training loss at epoch 320: 0.8308268189430237. Validation loss: 1.7472586631774902\n",
      "Training loss at epoch 330: 0.8239239454269409. Validation loss: 1.7452385425567627\n",
      "Training loss at epoch 340: 0.8180086612701416. Validation loss: 1.7432457208633423\n",
      "Training loss at epoch 350: 0.812210202217102. Validation loss: 1.7413651943206787\n",
      "Training loss at epoch 360: 0.8062543869018555. Validation loss: 1.7394369840621948\n",
      "Training loss at epoch 370: 0.800391674041748. Validation loss: 1.7376984357833862\n",
      "Training loss at epoch 380: 0.7953245043754578. Validation loss: 1.735888123512268\n",
      "Training loss at epoch 390: 0.790302038192749. Validation loss: 1.7342714071273804\n",
      "Training loss at epoch 400: 0.7866547703742981. Validation loss: 1.732758641242981\n",
      "Training loss at epoch 410: 0.7817161679267883. Validation loss: 1.7312015295028687\n",
      "Training loss at epoch 420: 0.7773224711418152. Validation loss: 1.7297618389129639\n",
      "Training loss at epoch 430: 0.773017168045044. Validation loss: 1.7282613515853882\n",
      "Training loss at epoch 440: 0.7698581218719482. Validation loss: 1.7268767356872559\n",
      "Training loss at epoch 450: 0.7664995789527893. Validation loss: 1.7254674434661865\n",
      "Training loss at epoch 460: 0.761749267578125. Validation loss: 1.7242728471755981\n",
      "Training loss at epoch 470: 0.7577333450317383. Validation loss: 1.7232029438018799\n",
      "Training loss at epoch 480: 0.7561242580413818. Validation loss: 1.721889853477478\n",
      "Training loss at epoch 490: 0.7521252632141113. Validation loss: 1.720711350440979\n"
     ]
    }
   ],
   "source": [
    "regular_cora = Planetoid(root = \"/home/jik19004/FilesToRun/AdversarialGNN\", name = \"Cora\", split = \"public\")\n",
    "regular_cora = regular_cora.data \n",
    "regular_model = GCN(in_channels = 1433, hidden_channels = 16, out_channels=7) \n",
    "Train_and_Evaluate(cora_transform, 500, np.inf, torch.nn.L1Loss, torch.device(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding $K_{x}$ and $K_{y}$ with GraphSage and optimal weight decay + dropout rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.79911373707533\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"/home/jik19004/FilesToRun/AdversarialGNN/LPGNN\")\n",
    "Evaluate(model, cora_transform, torch.device(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
